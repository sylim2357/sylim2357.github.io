---
title:  "Soft Actor-Critic Algorithms and Applications"
excerpt: "Haarnoja, Tuomas, et al. \"Soft actor-critic algorithms and applications.\" arXiv preprint arXiv:1812.05905 (2018)."
toc: true
toc_sticky: true

categories:
  - Paper Review
tags:
  - Machine Learning
  - Reinforcement Learning
  - Policy Gradient
last_modified_at: 2020-12-31T08:00:00+09:00
---

Arxiv [링크](https://arxiv.org/abs/1812.05905)

버클리와 구글에서 쓴 논문. 보통 액터-크리틱 기법은 온폴리시로 학습되지만, 이 논문에서는 오프폴리시 액터-크리틱 기법을 소개하고 최대 엔트로피를 적용했다. Soft Actor-Critic (SAC)을 소개한 [이전 논문](https://arxiv.org/abs/1801.01290)의 리패키지 에디션이다.

# Introduction

> Our method is, to our knowledge, the first off-policy actor-critic method in the maximum entropy reinforcement learning framework.

모델-프리 심층강화학습 기법이 실제 문제에 적용되는 데 크게 두가지 걸림돌이 있다. 먼저는 sample inefficiency라고 하는 문제로, 정책을 학습하는 데에 필요한 샘플 (경험)의 갯수가 환경의 복잡도에 대해서 기하급수적으로 많이 필요하다는 것이다. 또 하나는 이 기법이 하이퍼파라미터에 대해서 상당히 불안정하다는 것이다. 로보틱스 등 환경과 무한정으로 상호작용하기 힘든 실제 문제 상황에서는 특히나 큰 문제점으로 작용한다.

샘플 비효율성은 온폴리시 기법에서 특히 문제가 된다. 해당 정책을 따라서 상호작용을 하여 얻은 정보는 그 정책을 업데이트 하는 순간 쓸모가 없어지기 때문이다. 즉, 정책을 업데이트 하는 주기마다 새로운 정보가 지속적으로 필요하게 된다. 반면 오프폴리시 기법은 상호작용 데이터를 모아두었다가 정보를 추출할 수 있어 샘플 효율성에 이득이 있으나, 가치함수 근사방법으로 쓰는 뉴럴넷 등의 복잡한 함수는 하이퍼파라미터의 조정에 민감하게 반응한다는 단점을 가지고 있다. 특히 연속적인 상태/행동 도메인에서는 더 큰 문제가 된다.

샘플 효율성과 하이퍼파라미터 민감성을 둘 다 잡기 위해서 이 논문은 최대 엔트로피 방법 기반의 오프폴리시 기법인 Soft Actor-Critic을 제안한다. 오프폴리시이기 때문에 샘플 효율성이 뛰어나고, 최대엔트로피 기반의 방법이기 때문에 학습의 안정성을 확보할 수 있다.

저자는 SAC가 아래의 세 부분으로 이루어져 있다고 설명한다.

1. 정책과 가치를 학습하는 액터-크리틱 기반의 구조

2. 샘플 효율성을 위한 오프폴리시 기법

3. 안정성과 exploration을 위한 최대 엔트로피의 적용

오프폴리시 액터-크리틱 기법은 이번이 처음은 아니다, DDPG (Deep Deterministic Policy Gradient)의 경우에는 Q함수를 사용하여 오프폴리시를 이루었고, 이 Q함수를 최대화하는 deterministic action 함수를 학습하는 형태로 액터-크리틱을 구현하였다. 하지만, SAC는 stochastic actor를 사용하고 정책의 엔트로피를 최대화시키는 방안을 적용하여서 안정적인 학습과 나은 exploration을 얻었으며, 벤치마크 성능도 DDPG를 능가하였다.

# Preliminaries

## Notation

사용될 표기법과 가정을 알아본다.

MDP $$(\mathcal{S},\mathcal{A}, p, r)$$이 있다고 하자. 상태 공간 $$\mathcal{S}$$과 행동 공간 $$\mathcal{A}$$는 연속이고, 상태 전이 확률 $$p:\mathcal{S}\times \mathcal{S}\times\mathcal{A}\rightarrow[0,\infin)$$은 현재 상태 $$\bold{s}_t\in\mathcal{S}$$와 행동 $$\bold{a}_t\in\mathcal{A}$$가 주어졌을 때 다음 상태 $$\bold{s}_{t+1}\in\mathcal{S}$$로 전이될 확률을 나타낸다고 하자. 보상 $$r:\mathcal{S}\times\mathcal{A}\rightarrow[r_{min},r_{max}]$$은 각 전이에서 발생되며 유계이다. 또한 $$\rho_{\pi}(\bold{s}_t)$$와 $$\rho_{\pi}(\bold{s}_t,\bold{a}_t)$$는 정책 $$\pi(\bold{a}_t\mid\bold{s}_t)$$를 따를 때 생성되는 경로에서 $$\bold{s}_t$$와 $$(\bold{s}_t,\bold{a}_t)$$가 나타날 marginal probability를 나타낸다.

## Maximum Entropy Reinforcement Learning

보통 강화학습의 목표는 기대 보상의 합 $$\sum_{t}\mathbb{E}_{(\bold{s}_t,\bold{a}_t)\sim\rho_{\pi}}[r(\bold{s}_t,\bold{a}_t)]$$을 최대화시키는 정책 $$\pi(\bold{a}_t\mid\bold{s}_t)$$을 찾는 것이다. 이것을 일반화시킨 최대 엔트로피 강화학습은 **목표함수에 정책의 엔트로피를 포함시킨다.** 즉, 기대 보상을 최대화시킬 뿐 아니라 각 상태에서 택하는 정책의 엔트로피 또한 최대화해야하는 것이다. 최대 엔트로피 최적 정책은 아래와 같이 표현할 수 있다.

$$\pi^*=\text{arg}\underset{\pi}{\text{max}}\sum\limits_{t}\mathbb{E}_{(\bold{s}_t,\bold{a}_t)\sim\rho_{\pi}}[r(\bold{s}_t,\bold{a}_t)+\alpha\mathcal{H}(\pi(\cdot\mid\bold{s}_t))]$$

$$\mathcal{H}(\pi(\cdot\mid\bold{s}_t))$$는 정책 $$\pi$$를 따랐을 때 상태 $$\bold{s}_t$$에서 택할 행동의 확률분포의 엔트로피를 나타내는 것이다. 일반적으로 기대 보상의 합을 최대화시키려고 하면 정책은 deterministic해지려는 경향이 있기 때문에, 엔트로피를 최대화시키라는 목표와 상충된다. 높은 엔트로피를 갖기 위해서는 취할 수 있는 행동에 최대한 공평한 확률을 부여해야하기 때문이다. **보상은 뾰족한 정책을, 엔트로피는 평평한 정책을 만드려고 하는 것이다.** 즉, 엔트로피 항 앞에 있는 $$\alpha$$라는 하이퍼파라미터는 (temperature parameter라고 불린다.) 정책의 랜덤성을 결정짓는다. $$\alpha=0$$일 경우에는 전통적인 강화학습 문제로 돌아갈 수 있다. Infinite horizon문제일 경우 discount factor $$\gamma$$를 적용해야 하는데, 아래와 같이 formulate될 수 있다.

$$J(\pi)=\sum\limits^{\infin}_{t=0}\mathbb{E}_{(\bold{s}_t,\bold{a}_t)\sim\rho_\pi}\Bigg[\sum\limits^{\infin}_{l=t}\gamma^{l-t}\mathbb{E}_{\bold{s}_l\sim p,\bold{a}_l\sim \pi}[r(\bold{s}_t,\bold{a}_t)+\mathcal{H}(\pi(\cdot\mid\bold{s}_t))\mid\bold{s}_t,\bold{a}_t]\Bigg]$$

풀어서 설명하면, 모든 상태-행동 쌍에서 시작할 때 얻게 되는 기대 보상과 엔트로피를 할인하여 합한 것을 시작 상태-행동 분포에 따라서 기댓값을 구한 것의 합이다. 현재 정책 $$\pi$$ 하의 상태-행동 쌍은 분포 $$\rho_{\pi}$$를 따른다.

이와 같이 최대 엔트로피를 강화학습 목표에 적용했을 때 얻는 이점이 크게 세 가지가 있다. 첫 번째로, 일반 강화학습 목표를 최대화하려고 할 때보다 더 탐험을 많이 하면서 가망이 없어보이는 행동은 과감히 포기한다. 현재 최적이라고 생각하지는 않지만 가능성이 있어보이는 행동을 취할 확률이 높아진다는 것이다. 둘째로, 최적에 가까운 행동을 여러개 찾는 효과를 보게 된다.

정리하자면, 정책의 엔트로피를 최대화하는 동시에 보상도 챙겨야 하기 때문에 **기대 보상이 높을 것 같은 행동을 여러 개 찾아내어 비슷한 확률을 배정하고, 기대 보상이 낮을 것 같은 행동들은 일찍이 포기**해버리는 결과를 보게 된다. 마지막으로는 학습 속도의 개선이 있었다고 한다.

## Soft Q-Learning

저자가 쓴 이전 [논문](https://arxiv.org/abs/1702.08165)에서도 엔트로피 기반 강화학습이 사용되었다. 하지만 이전 논문은 Q학습에 analogous한 soft Q-Learning을 제시함으로서 $$Q^*_{soft}$$와 $$V^*_{soft}$$을 찾는 부분이 포함되어 있다. $$Q_{soft}$$와 $$V_{soft}$$는 아래에서 정의할 것이나, 우리가 알고있는 가치함수에 정책의 엔트로피를 더한 것이라고 생각하면 된다. Soft Q-Learning 논문에서 정의한 $$Q^*_{soft}$$와 $$V^*_{soft}$$, 이를 바탕으로 정의한 soft Bellman Equation 모두 최적을 잘 찾아간다고 증명되었으나, 모든 가치 기반 기법이 그렇듯 Q값을 과대추측하는 경향이 있다. Positive bias가 있는 것이다. Bellman backup에서 Jensen's Inequality를 사용하면 아래와 같은 식을 도출할 수 있다.

$$⁍$$

윗줄에 나와있는 $$\text{max}\,\mathbb{E}$$표현이 실제 Q값이고, 아랫줄에 있는 표현이 업데이트를 통해 얻는 값이다. 즉, 실제 구하고자하는 값의 상한으로 Q를 업데이트하고 있는 것이다. 업데이트가 진행될수록 오차는 쌓이게 된다. 이를 해결하기 위해서 SAC에서는 clipped double Q-trick이라고 불리는 방법을 사용한다. Double Q-Learning에서 사용한 방법을 연속적인 행동 도메인으로 확장시킨 것이라고 생각하면 된다.

또한, soft Q-Learning에서는 정책함수에 에너지 기반 함수를 사용한다. $$exp(-\mathcal{E}(\bold{s}_t,\bold{a}_t))$$라고 표기되는 정책함수는 intractable하기 때문에 함수 근사와 행동 샘플링하는 데에 애를 먹어야 한다. Soft Q-Learning에서는 이 때문에 Stein Variational Gradient Descent를 활용해 함수 근사를 진행하고 샘플링을 위해서도 근사적인 방법을 사용하였다. 오차가 있는 것은 물론 연산량도 늘어난다. Soft Actor-Critic에서는 정책함수를 tractable하게 유지하면서 최대한 직접적으로 근사하려고 하였다.

# Model

## Soft Policy Iteration

> We instead evaluate the Q-function of the current policy and update the policy through an off-policy gradient update.

Soft Q-Learning때와 마찬가지로 Soft Policy Iteration을 정의한다. 기존 policy iteration처럼 policy evaluation과 policy improvement 단계로 이루어져 있다. 행동이 countable finite이고 정책이 parameterized density인 경우를 가정하여 먼저 살펴본다.

**Policy Evaluation step**에서는 최대 엔트로피 목표를 최대화하는 정책 $$\pi$$를 따랐을 때의 가치를 계산한다. $$\pi$$가 고정되어있다고 가정하고 soft Q-value 업데이트를 하면 $$Q^{\pi}$$를 얻을 수 있다. 이때 사용하는 Bellman backup operator $$\mathcal{T}^{\pi}$$는 아래와 같이 주어진다.

$$\mathcal{T}^{\pi}Q(\bold{s}_t,\bold{a}_t)\triangleq r(\bold{s}_t,\bold{a}_t)+\gamma\,\mathbb{E}_{\bold{s}_{t+1}\sim p}[V(\bold{s}_{t+1})]\newline V(\bold{s}_t)=\mathbb{E}_{\bold{a}_t\sim\pi}[Q(\bold{s}_t,\bold{a}_t)-\alpha \log\pi(\bold{a}_t\mid\bold{s}_t)]$$

Bellman Operator는 전통적인 강화학습과 같은 대신 soft value function $$V(\bold{s}_t)$$에 엔트로피 항이 추가된 것을 볼 수 있다. Statement를 정리하자면,

**Lemma 1**. 위와 같이 정의된 soft Bellman backup operator $$\mathcal{T}^{\pi}$$가 있고, $$\lvert\mathcal{A}\rvert<\infin$$인 경우에 함수 $$Q^0:\mathcal{S}\times\mathcal{A}\rightarrow\mathbb{R}$$가 있다고 하자. $$Q^{k+1}=\mathcal{T}^\pi Q^k$$라고 한다면 $$Q^k$$의 수열은 $$Q^\pi_{soft}$$ ($$\pi$$에서의 soft Q값)으로 수렴한다.

$$r_\pi(\bold{s}_t,\bold{a}_t)\triangleq r(\bold{s}_t,\bold{a}_t)+\mathbb{E}_{\bold{s}_{t+1}\sim p}[\mathcal{H}(\pi(\cdot\mid\bold{s}_{t+1}))]$$이라고 정의하고 아래와 같은 업데이트를 진행한다면 일반 policy evaluation에서 보인 것과 같은 결과를 보일 수 있다.

$$Q(s,a)\leftarrow r_\pi(s,a)+\gamma\,\mathbb{E}_{\bold{s}_{t+1}\sim p,\,\bold{a}_{t+1}\sim \pi}[Q(\bold{s}_{t+1},\bold{a}_{t+1})]$$

$$\mid\mathcal{A}\mid<\infin$$은 엔트로피를 적용한 보상 $$r_\pi$$가 유계이기 위해 필요한 조건이다. 즉, soft Q value는 아래와 같이 정의된다고 이해할 수 있다.

$$Q^\pi(\bold{s}_t,\bold{a}_t)=r(\bold{s}_t,\bold{a}_t)+\mathbb{E}_{(\bold{s}_{t+1},\bold{a}_{t+1},\dots)\sim p_\pi}\bigg[\sum\limits^{T}_{l=1}r(\bold{s}_{t+l},\bold{a}_{t+l})-\alpha\log\pi(\bold{a}_{t+l}\mid\bold{s}_{t+l})\bigg]$$

**Policy Improvement step**에서는 soft Q function의 exponential과 비례하게 정책을 업데이트 한다. 이렇게 할 경우 각 업데이트 단계마다 더 나은 정책을 얻게 되며, tractable한 정책을 얻기 위해 parameterized distribution family $$\Pi$$에 속하는 정책만을 고려한다. 각 단계마다 $$\Pi$$에 속한 정책 중 soft Q function과 가장 가까운 것으로 정책을 업데이트 한다. 위에서 본 $$J$$를 전개해보면 아래와 같이 나온다.

$$J(\pi)=\mathbb{E}_{(\bold{s}_0,\bold{a}_0)}[Q^\pi(\bold{s}_0,\bold{a}_0)-\alpha\log\pi(\bold{a}_0\mid\bold{s}_0)]\newline=-\mathbb{E}_{\bold{s}_0}\bigg[\text{D}_\text{KL}\bigg(\pi(\cdot\mid\bold{s}_0)\Bigg\|\text{exp}\bigg(\frac{1}{\alpha}Q^\pi(\bold{s}_0,\cdot)\bigg)\bigg)\bigg]+constant$$

위의 KL발산을 줄이는 정책을 찾는 것이 목표함수를 최대화하는 것과 같은 효과이다. 즉, 아래와 같은 목표를 얻을 수 있다.

$$\pi_{\text{new}}=\text{arg}\underset{\pi'\in\Pi}{\text{min}}\,\text{D}_\text{KL}\Bigg(\pi'(\cdot\mid\bold{s}_t)\Bigg\|\frac{\text{exp}(\frac{1}{\alpha}Q^{\pi_{\text{old}}}(\bold{s}_t,\cdot))}{Z^{\pi_{\text{old}}}(\bold{s}_t)}\Bigg)$$

$$Z^{\pi_{\text{old}}}$$는 분포를 normalizing해주는 표현으로, intractable하지만 그래디언트에 기여하는 바가 없기 때문에 무시해도 괜찮다. 이 내용을 정리하면,

**Lemma 2**. $$\pi_{\text{old}}\in\Pi$$이고 위와 같이 정의된 $$\pi_{\text{new}}$$가 있다고 하자. $$\lvert\mathcal{A}\rvert<\infin$$일 때 모든 $$(\bold{s}_t,\bold{a}_t)\in\mathcal{S}\times\mathcal{A}$$에 대해서 $$Q^{\pi_{\text{new}}}(\bold{s}_t,\bold{a}_t)\geq Q^{\pi_{\text{old}}}(\bold{s}_t,\bold{a}_t)$$가 성립한다.

위에서 살펴본 Policy Evaluation이랑 Policy Improvement을 번갈아서 적용하면 최적의 정책을 찾을 수 있다. Soft Policy Iteration이라고 이름붙인 이 기법을 정리하면 아래와 같다.

**Theorem 1**. 어떠한 $$\pi\in\Pi$$에 Policy Evaluation과 Policy Improvement을 적용하면 $$\lvert\mathcal{A}\rvert<\infin$$일 때 모든 $$(\bold{s}_t,\bold{a}_t)\in\mathcal{S}\times\mathcal{A}$$에 대해서 $$Q^{\pi^*}(\bold{s}_t,\bold{a}_t)\geq Q^{\pi}(\bold{s}_t,\bold{a}_t)$$을 만족하는 $$\pi^*\in\Pi$$에 수렴한다.

위의 정리가 성립하기 위해서는 countable finite하다는 tabular setting이 있어야만 한다. 실제 문제에 적용하기 위해서는 continuous domain에 적용해야 하고, 최적 값에 수렴할때까지 두 방법을 사용하는 것은 연산량이 너무 많기 때문에 Soft Actor Critic이라는 실용적인 알고리즘을 제안한다.

## Soft Actor-Critic

Policy Evaluation과 Policy Improvement를 적용하는 것은 수렴할때까지 너무 오랜 시간이 걸리기 때문에 soft Q함수와 soft policy함수를 나타내는 function approximator를 각각 만든 후 각 함수를 번갈아서 피팅한다. Soft Q함수는 $$\theta$$라는 파라미터를 가지는 뉴럴넷 $$Q_\theta(\bold{s}_t,\bold{a}_t)$$로 근사하고, soft 정책함수는 $$\phi$$를 파라미터로 가지는 tractable 분포 $$\pi_{\phi}(\bold{a}_t,\bold{s}_t)$$로 근사한다. 정책함수는 가우시안을 따르게 만들고, 평균과 공분산을 뉴럴넷으로 모델링한다.

**Soft Q-function**은 Soft Bellman residual을 최소화하는 방향으로 학습한다. 손실함수는 아래와 같다.

$$J_Q(\theta)=\mathbb{E}_{(\bold{s}_t,\bold{a}_t)\sim \mathcal{D}}\bigg[\frac{1}{2}(Q_\theta(\bold{s}_t,\bold{a}_t)-(r(\bold{s}_t,\bold{a}_t)+\gamma\mathbb{E}_{\bold{s}_{t+1}\sim p}[V_{\bar\theta}(\bold{s}_t)]))^2\bigg]$$

위 식의 그래디언트를 계산하면 아래와 같은 식이 나온다.

$$\hat\nabla_\theta J_Q(\theta)=\nabla_\theta Q_\theta(\bold{s}_t,\bold{a}_t)\bigg(Q_\theta(\bold{s}_t,\bold{a}_t)-(r(\bold{s}_t,\bold{a}_t)+\gamma(Q_{\bar\theta}(\bold{s}_t,\bold{a}_t)-\alpha\log(\pi_\phi(\bold{a}_t\mid\bold{s}_t)))\bigg)$$

여기서 target부분에 $$\bar\theta$$가 들어가있는 것을 볼 수 있는데, 이전 스텝들에서 얻은 $$\theta$$의 exponentially moving average를 사용한 것이다. 안정적인 학습을 위해 사용되었다.

**Soft Policy function**은 ****위에서 본 KL발산 표현을 줄이는 방향으로 학습된다. 손실함수는 아래와 같다. 각 상태에 대해서 KL발산을 줄여야 하기 때문에 기댓값이 두 개 들어간 것을 볼 수 있다. KL발산의 정의를 사용하여 전개하면 쉽게 얻을 수 있다.

$$J_\pi(\phi)=\mathbb{E}_{\bold{s}_t\sim \mathcal{D}}[\mathbb{E}_{\bold{a}_t\sim\pi_\phi}[\alpha\log\pi_\phi(\bold{a}_t\mid\bold{s}_t)-Q_\theta(\bold{s}_t,\bold{a}_t)]]$$

Target으로 쓰이는 함수가 Q함수이기 때문에, $$\pi$$또한 Q처럼 뉴럴넷으로 모델링하여 같은 방식으로 학습하는게 편할 것이다. Reparameterization trick을 사용하여 $$\bold{a}_t=f_\phi(\epsilon_t;\bold{s}_t)$$로 모델링한다. 가우시안에서 샘플링한 노이즈 $$\epsilon_t$$를 포함하는 뉴럴넷이다. 이제 손실함수는 다음과 같이 표현될 수 있다.

$$J_\pi(\phi)=\mathbb{E}_{\bold{s}_t\sim \mathcal{D},\epsilon_t\sim\mathcal{N}}[\alpha\log\pi_\phi(f_\phi(\epsilon_t;\bold{s}_t)\mid\bold{s}_t)-Q_\theta(\bold{s}_t,f_\phi(\epsilon_t;\bold{s}_t))]$$

$$\pi_\phi$$는 $$f_\phi$$로 나타낼 수 있다. 이제 위 식의 그래디언트를 아래와 같이 근사한다.

$$\hat\nabla_\phi J_\pi(\phi)=\nabla_\phi\alpha\log(\pi_\phi(\bold{a}_t\mid\bold{s}_t))+(\nabla_{\bold{a}_t}\alpha\log(\pi_\phi(\bold{a}_t\mid\bold{s}_t))-\nabla_{\bold{a}_t}Q(\bold{s}_t,\bold{a}_t))\nabla_\phi f_\phi(\epsilon_t;\bold{s}_t)$$

$$\bold{a}_t$$는 $$f_\phi(\epsilon_t;\bold{s}_t)$$의 값을 사용한다. $$\hat\nabla_\phi J_\pi(\phi)$$는 실제 그래디언트의 불편추정량이다.

이전 SAC논문에서는 soft value function을 추정하는 모델이 하나 더 있었으나 불필요하다고 판단되어서 이번 논문에서는 빠졌다. Soft Actor-Critic에서 나오는 모든 $$\mathcal{D}$$는 replay memory에서 샘플링해서 하는 것으로 offline학습이 가능하다.

여기까지 내용이 첫 SAC [논문](https://arxiv.org/abs/1801.01290)에서 다룬 내용이다. 하이퍼파라미터에 대해서 상당히 강건하다는 결론을 냈었는데 유일한 단점이 바로 $$\alpha$$, temperature parameter에 대해서는 민감하게 반응한다는 것이었다. stochasticity를 조금만 낮추어도 뾰족한 정책이 나오고, 조금만 높여도 평평한 정책이 나왔기 때문이다. 이 문제를 해결하고자 이 논문에서는 아래와 같이 하이퍼파라미터 $$\alpha$$도 최적화 문제를 품으로서 학습하는 전략을 취했다.

## Automating Entropy Adjustment for Maximum Entropy RL

최대 엔트로피 강화학습 문제의 목표는 정책의 엔트로피를 포함한다. 매번 행동을 내리고 보상을 받을 때마다 정책은 업데이트 되는데, 그말인즉슨 정책이 업데이트 될 때마다 정책의 엔트로피가 바뀌고 강화학습 문제의 목표 또한 바뀐다는 것이다. Temperature parameter $$\alpha$$를 정하기 어려운 이유가 여기에 있는 것이다. 애초에 랜덤성을 고정시킨다는 것이 안좋은 생각이다.

강화학습 문제의 목표를 새로 정의함으로서 다른 접근법을 취한다. 결국 원하는 것은 매 단계마다 결정을 내릴 때 기대 보상을 최대화 하고 싶은 것은 맞으나, 어느 정도의 엔트로피 제약조건을 고려하고 싶은 것이기 때문이다. 따라서, 강화학습 문제를 아래와 같은 제약 최적화 문제로 정의할 수 있다.

$$\underset{\pi_0}{\text{max}}\,\mathbb{E}_{\rho_\pi}\bigg[\sum\limits^{T}_{t=0} r(\bold{s}_t,\bold{a}_t)\bigg] \\ \text{s.t.} \quad \mathbb{E}_{(\bold{s}_t,\bold{a}_t)\sim\rho_\pi}[-\log(\pi_t(\bold{a}_t\mid\bold{s}_t))]\geq\mathcal{H}\quad\forall t$$

$$\mathcal{H}$$라는 엔트로피 하한을 만족하면서 기대 보상 합을 최대화하라는 것이다. Fully Observed MDP에서는 deterministic한 정책이 최적이기 때문에 하한 $$\mathcal{H}$$의 존재 자체가 tight한 조건을 주는 것이라고 이해할 수 있다. 이제 거꾸로 풀어 들어가는 동적계획법 접근을 취해서 기대보상 합을 최대화시키는 문제를 재귀적으로 전개해보자.

$$\underset{\pi_0}{\text{max}}\bigg(\mathbb{E}[r(\bold{s}_t,\bold{a}_t)]+\underset{\pi_1}{\text{max}}\bigg(\mathbb{E}[\dots]+\underset{\pi_T}{\text{max}}\,\mathbb{E}[r(\bold{s}_T,\bold{a}_T)]\bigg)\bigg)$$

위와 같은 식이 될 것이다. 마지막 제약 최적화문제를 생각해보자.

$$\underset{\pi_T}{\text{max}}\,\mathbb{E}_{(\bold{s}_t,\bold{a}_t)\sim\rho_\pi}[r(\bold{s}_T,\bold{a}_T)]\\ \text{s.t.}\quad\mathbb{E}_{(\bold{s}_T,\bold{a}_T)\sim\rho_\pi}[-\log(\pi_T(\bold{a}_T\mid\bold{s}_T))]\geq\mathcal{H}$$

쌍대변수 $$\alpha_T$$를 소개하여 강한 쌍대성 원리를 적용하게 되면 위의 목적함수는 아래와 같이 된다.

$$\underset{\alpha_T\geq0}{\text{min}}\,\underset{\pi_T}{\text{max}}\,\mathbb{E}[r(\bold{s}_T,\bold{a}_T)-\alpha_T\log\pi(\bold{a}_T\mid\bold{s}_T)]-\alpha_T\mathcal{H}$$

목적함수가 선형이고 제약함수가 엔트로피에 대해서 볼록이기 때문에 가능한 것이다. 여기서 얻을 수 있는 최적 정책은 $$\alpha_T$$가 주어졌을 때의 최적 정책 $$\pi^*_T(\bold{a}_T\mid\bold{s}_T;\alpha_{T})$$이다.

이젠 $$T-1$$번째의 제약 최적화 문제를 생각해보자. $$T$$와 $$T-1$$번째 정책의 엔트로피 제약조건을 고려했을 때 아래의 목적을 이루는 것이다.

$$\underset{\pi_{T-1}}{\text{max}}\bigg(\mathbb{E}[r(\bold{s}_{T-1},\bold{a}_{T-1})]+\underset{\pi_T}{\text{max}}\,\mathbb{E}[r(\bold{s}_T,\bold{a}_T)]\bigg)$$

여기서 아래와 같은 soft Q-function 표현을 위에 대입하고 쌍대성 원리를 다시 적용하면,

$$Q^*_t(\bold{s}_t,\bold{a}_t;\pi^*_{t+1:T},\alpha^*_{t+1:T})=\mathbb{E}[r(\bold{s}_t,\bold{a}_t)]+\\ \qquad \qquad \qquad \qquad \qquad \qquad \mathbb{E}_{\rho_\pi}[Q^*_{t+1}(\bold{s}_{t+1},\bold{a}_{t+1})-\alpha^*_{t+1}\log\pi^*_{t+1}(\bold{a}_{t+1}\mid\bold{s}_{t+1})]$$

$$\underset{\pi_{T-1}}{\text{max}}\bigg(\mathbb{E}[r(\bold{s}_{T-1},\bold{a}_{T-1})]+\underset{\pi_T}{\text{max}}\,\mathbb{E}[r(\bold{s}_T,\bold{a}_T)]\bigg)\\=\underset{\pi_{T-1}}{\text{max}}\bigg(Q^*_{T-1}(\bold{s}_{T-1},\bold{a}_{T-1})-\alpha^*_T\mathcal{H}(\pi^*_T)\bigg)\\=\underset{\alpha_{T-1}\geq0}{\text{min}}\,\underset{\pi_{T-1}}{\text{max}}\bigg(\mathbb{E}[Q^*_{T-1}(\bold{s}_{T-1},\bold{a}_{T-1})]-\mathbb{E}[\alpha_{T-1}\log\pi(\bold{a}_{T-1}\mid\bold{s}_{T-1})]-\alpha_{T-1}\mathcal{H}\bigg)-\alpha^*_{T}\mathcal{H}(\pi^*_T)$$

이러한 문제로 다시 만들 수 있으며, 바로 위에서 다룬 $$T$$번째 최적화 문제를 푸는것과 동일한 형태가 된다. 결국 $$\alpha$$를 어떻게 구할 수 있느냐.

$$\alpha^*_{t}=\text{arg}\underset{\alpha_t}{\text{min}}\mathbb{E_{\bold{a}_t\sim\pi^*_t}}[-\alpha_t\log\pi^*_t(\bold{a}_t\mid\bold{s}_t;\alpha_t)-\alpha_t\bar\mathcal{H}]$$

매 단계의 최적화 문제를 풀 때마다 위와 같은 식의 $$\alpha$$를 구하면 된다는 것을 알 수 있다. 마지막 문제부터 풀어서 제일 바깥쪽에 있는 문제까지 계산해낼 수 있는 것이다.

## Practical Algorithm

종합하여 알고리즘을 살펴보자.

![](/assets/images/sac-01.png)

먼저, 앞서 설명한 가치기반 알고리즘의 고질적인 문제점인 value overestimation을 해결하기 위해서 soft Q-function을 두 개를 사용하였다. 두 개의 Q함수를 각각 $$J_Q(\theta_i)$$를 최소화하는 방향으로 업데이트 하면서 두 함수 중 최소값을 반환하는 함수를 Q함수 학습에 필요한 그래디언트를 구하는 부분과 정책함수 학습에 사용한다.

또한, 위에서 설명한 $$\alpha$$를 구하는 이론적인 방법은 사실 현실적이지 않다. 몇 개의 최적화 문제를 풀어야 하는지 생각해보면 알 수 있다. 대신, dual gradient descent라는 기법을 사용한다. 라그랑지안을 원문제 변수와 쌍대문제 변수에 대해서 각각 수렴할때까지 번갈아서 경사하강을 실시하는 방법이다. $$\alpha$$에 대해서 경사하강을 실시해야 한다는 말이기도 한데, 아래의 손실함수를 사용하여 $$\alpha$$를 학습한다.

$$J(\alpha)=\mathbb{E}_{\bold{a}_t\sim\pi_t}[-\alpha\log\pi_t(\bold{a}_t\mid\bold{s}_t)-\alpha\bar\mathcal{H}]$$

결국 위에 나와있는 알고리즘을 보면, 현재 정책을 따라가면서 상호작용 정보를 얻는 단계와 replay buffer에서 샘플링한 batch를 통해 function approximator들을 업데이트하는 단계를 번갈아서 진행한다.

# My Conclusion

오랜만에 최적화랑 강화학습 전반에 대해서 다시 복습해야 이해할 수 있는 논문이었던 것 같다.
